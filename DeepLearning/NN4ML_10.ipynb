{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Machine Learning (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why it Helps to Combine Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bias-variance trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Squared error:\n",
    "- Bias term: big if model has to little capacity\n",
    "- Variance term: big if model has to much capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If individual predictors diagree, combining them will yield better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average of all predictors: $\\bar{y}=<y_i>i=\\frac{1}{N}\\sum^N_{i=1}y_i=(t-\\bar{y})^2+<(y_i-\\bar{y})^2>$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Discrete distributions over class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "average better than random: $log(\\frac{p_i+p_j}{2})\\ge\\frac{logp_i+logp_j}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ways to make predictors differ\n",
    "- Algorithms stuck in different local optima\n",
    "- Different models\n",
    "- Neural network\n",
    " - different number of hidden layers\n",
    " - different number of units\n",
    " - different types of strengths\n",
    " - different learning algorithms\n",
    "- Different training data\n",
    " - Bagging\n",
    " - Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixtures of Experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we do better than just averaging models in a training case independent manner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple models: good if dataset contains different regimes\n",
    "- Need to cluster data into clusters\n",
    " - Want to cluster by similarites between input and output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Error function encourages cooperation\n",
    "$E=(t-<y_i>_i)^2$ -> will overfit badly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Error function encourages specialization\n",
    "E = $<p_i(t-y_i)^2>_i$ -> Each expert will learn to do good on subset of training cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple cost function: $E=\\sum_ip_i(t-y_i)^2$  \n",
    "Better but complex cost function for mixtures: $p(t^c|MoE)=\\sum_ip_i^c\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(t^c-y^c)^2}$ -> cost:$-logp(t^c|MoE)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Idea of Full Bayesian Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find full posterior distribution over all possible parameter settings -> it's extreamly computationally intensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/10-3-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Approximate full bayesian learning in NN\n",
    "put a grid over parameter space, and evaluate p(W|D) at each grid-point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(t_{test}|input_{test})=\\sum_{g\\in grid}p(W_g|D)p(t_{test}|input_{test},W_g)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Full Bayesian Learning Practical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: might be good enough just to sample weight vectors  \n",
    "$p(y_{test}|input_{test},D)=\\sum_ip(W_i|D)p(y_{test}|input_{test},W_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/10-4-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Chain Monte Carlo -> makes it feasible to use full Bayesian learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Full Bayesian learning with mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Two ways to average models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Mixture: combine by averaging their output probabilities\n",
    "2. Product: take geometric mean of their output probabilites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:bhplayground]",
   "language": "python",
   "name": "conda-env-bhplayground-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
