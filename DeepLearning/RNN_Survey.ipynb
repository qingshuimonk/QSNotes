{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Critical Review of Recurrent Neural Networks for Sequence Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a [paper](https://arxiv.org/pdf/1506.00019.pdf) reading note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recurrent Neural Networks(RNN)**: are connectionist models with the ability to selectively pass information across the sequence steps, while processing sequential data one element at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Not Using Markov Models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Their states must be drawn from a modestly sized discrete state space $S$, the inference scales in time $O(|S|^2)$\n",
    "- Transition matrix grow in size $|S|^2$\n",
    "- Only takein previous one state, have much lager matrices if count into more previous states\n",
    "- Rendering Markov models computationally impractical for modeling long-range dependencies **(NILM?)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$v_j=l_j(\\sum_{j'}w_{jj'}v_{j'})$  \n",
    "- $jj'$ denotes \"to-from\"\n",
    "- $v_j$: output of node $j$\n",
    "- $l_j$: activation function of node $j$\n",
    "- $w_{jj'}$: weight\n",
    "- $v_{j'}$: output of node $j'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term the weighted sum inside parentheses the _incoming activation_, note as $a_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common activation functions:\n",
    "- Sigmoid: $\\delta(z)=\\frac{1}{1+e^{-z}}$\n",
    "- tanh: $\\phi (z)=\\frac{e^z-e^{-z}}{e^z+e^{-z}}$\n",
    "- Rectified Linear Unit(ReLU): $l_j(z)=max(0,z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output activation function: softmax\n",
    "- $\\hat{y}_k=\\frac{e^{a_k}}{\\sum_{k'=1}^Ke^{a_{k'}}}$ for $k=1$ to $k=K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN: feedforward NN augmented by the inclusion of edges that span adjacent time steps  \n",
    "At time $t$, nodes with recurrent edges receive input from both $x^{(t)}$ and $h^{(t-1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h^{(t)}=\\delta(W^{hx}x^{(t)}+W^{hh}h^{(t-1)}+b_h)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y}^{(t)}=softmax(W^{yh}h^{(t)}+b_y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics\\RNN_Survey_1.png)\n",
    "![](pics\\RNN_Survey_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Training RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extremely difficult:\n",
    "- vanishing\n",
    "- exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Truncated backpropagation through time**: on solution to exploding/vanishing gradients -> sacrifice the ability to have long memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/RNN_survey_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each ordinary node in hidden layer is replaced by a memory cell -> recurrent edge with weight 1: ensure gradient can pass without vanishing or exploding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/RNN_survey_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/RNN_survey_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full algorithm of LSTM with forget gates:\n",
    "- $g^{(t)}=\\phi(W^{gx}x^{(t)}+W^{gh}h^{(t-1)}+b_g)$\n",
    "- $i^{(t)}=\\delta(W^{ix}x^{(t)}+W^{ih}h^{(t-1)}+b_i)$\n",
    "- $f^{(t)}=\\delta(W^{fx}x^{t}+W^{fh}h^{t-1}+b_f)$\n",
    "- $o^{(t)}=\\delta(W^{ox}x^{t}+W^{oh}h^{t-1}+b_o)$\n",
    "- $s^{(t)}=g^{(g)}\\odot i^{(i)}+s^{(t-1)}\\odot f^{(t)}$\n",
    "- $h^{(t)}=\\phi(s^{(t)})\\odot o^{(t)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional Recurrent Neural Networks (BRNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/RNN_survey_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "has two layers of hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $h^{(t)}=\\delta(W^{hx}x^{(t)}+W^{hh}h^{(t-1)}+b_h)$\n",
    "- $z^{(t)}=\\delta(W^{Zx}x^{(t)}+W^{ZZ}z^{(t+1)}+b_z)$\n",
    "- $\\hat{y}^{(t)}=softmax(W^{yh}h^{(t)}+W^{yz}z^{(t)}+b_y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:bhplayground]",
   "language": "python",
   "name": "conda-env-bhplayground-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
