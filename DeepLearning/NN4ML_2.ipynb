{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Machine Learning (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture: the way neurons connect together\n",
    "#### Feed-forward Neural Networks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![A demo of feed-forward neural networks](https://www.researchgate.net/profile/Ramon_Quiza/publication/234055177/figure/fig1/AS:300092981563410@1448559150651/Figure-61-Sample-of-a-feed-forward-neural-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The first is the input layer and the last is the output layer\n",
    "2. \"Deep\" -> more than one hidden layers\n",
    "3. Activation function is non-linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![A demo of recurrent neural networks](https://www.researchgate.net/profile/Yen-Ming_Chiang/publication/222562767/figure/fig2/AS:305112384851969@1449755869325/Fig-2-The-architecture-of-a-recurrent-neural-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Directed cycles in connection graph\n",
    "2. Complecated dynamics -> difficult to train\n",
    "3. Natural way to modern sequential data -> can remmember information in hidden states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symmetrically Connected Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN with same weights on both directions -> can't model cycles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons: the First Generation of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard way to recognize patterns: \n",
    "1. Convert input vector to a vector of feature activations\n",
    "2. Learn how to weight each feature activation to get a single value\n",
    "3. Take a threshold of that single value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision units:\n",
    "- McCulloch-Pitts (step function): $z=b+\\sum_ix_iw_i$\n",
    "- A bias is a weight for the input which always have 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence procedure for perceptron:\n",
    "1. Add bias\n",
    "2. Pick training cases (make sure each one is picked)\n",
    " 1. Do nothing if output is right\n",
    " 2. Add input to weight vector if incorrectly output 0\n",
    " 3. Substract input to weight vector if incorrectly output 1\n",
    "It is guaranteed to converge if there is such set of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Geometric View of Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight space:\n",
    "- one dimension\n",
    "- a point represents a setting of weights\n",
    "- each train case is a hyperplane through origin (thresh at 0)\n",
    "- the weight vector need to be at the correct side (scalar product between weight vector and input vector should match the correct answer)\n",
    "Converge: find a point that at right side for all inputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why the Learning Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider squre distance $d^2_a+d^2_b$ between any feasible weight vector and current weight vector  \n",
    "Update weight vector closer to feasible weight vector every time perceptron makes a mistake  \n",
    "Define \"geneoursly feasible\": at least the input vector length away from hyper plane at the correct side  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Perceptrons Can't Do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hand-code feature will limit the adapatability of perceptron\n",
    "- Limitation of linear boundary (can't deal with not linear-seperable cases)\n",
    "- Can't discriminate patterns with same number of on pixels (translation with wraparound): each pixel will have same number of weights summed in both pattern A and pattern B  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal for pattern recognition is to learn the patterns despite of the transformations  \n",
    "Networks without hidden units are limited to input-output mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
