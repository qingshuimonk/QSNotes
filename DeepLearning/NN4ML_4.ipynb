{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Machine Learning (4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Brief Diversion into Cognitive Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What it means to have a concept:\n",
    "- Feature theory: a set of semantic features\n",
    "- The structural list theory: meaning lies in its relation to other concepts (relational graph)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add them togehter: neural net can use vector of semantic features build relational graph\n",
    "- No explicit inference is required in learning of neural net\n",
    "- The net \"intuit\" the answer in a forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Localist representation:  \n",
    "1. Treat neuron as a node in relational graph\n",
    "2. Treat connect as a binary relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems:\n",
    "1. Need many types of relations\n",
    "2. Connections might not have discrete values\n",
    "3. Also need ternary relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed representation:\n",
    "1. Many neurons used for each concept\n",
    "2. Each neuron involved in many concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Diversion: the Softmax Output Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawbacks of squared error:\n",
    "1. There will be almost no gradient if output is off the target too much\n",
    "2. The outputs should sum to 1 if we want to assign probability to mutually exclusive classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better cost function: softmax function (force cost function represent a probability distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax (soft continuous version of max function): $y_i=\\frac{e^{z_i}}{\\sum_{j\\in group}e^{z_j}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics\\4-3-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice feature: $\\frac{\\partial{y_i}}{\\partial{z_i}}=y_i(1-y_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Right cost function: negative log probabity of the correct answer (cross-entropy cost function)  \n",
    " $C=-\\sum_jt_jlogy_j$  \n",
    " - Very large gradient\n",
    " - Steepness of dC/dy balances dy/dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial{C}}{\\partial{z_i}}=\\sum_j\\frac{\\partial{C}}{\\partial{y_j}}\\frac{\\partial{y_j}}{\\partial{z_i}}=y_i-t_i$  \n",
    "Slope only become small when output close to target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuro-probabilistic Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speech recognition system need to know which work comes next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigram method: count frequencies of all triples of words  \n",
    "$\\frac{p(w_3=c|w_2=b,w_1=a)}{p(w_3=d|w_2=b,w_1=a)}=\\frac{count(abc)}{count(abd)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disadvantage: trigram method cannot understand similarity of words  \n",
    "-> we need convert the semantic and syntatic of previous words into feature vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/4-4-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: too many words = too many outgoing weights in last hidden unit  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways to Deal with the Large Number of Possible Outputs in Neuro-probabilistic Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A serial Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![](pics\\4-5-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compute score for each candidate word\n",
    "2. Use all logits in softmax\n",
    "3. Get cross-entropy derivatives\n",
    "4. Use other predictor to use a small set of candidate word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use a Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/4-5-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximize the prob of target word = maximize the branches on the path to the word -> log(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Much Simpler Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![](pics\\4-5-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:bhplayground]",
   "language": "python",
   "name": "conda-env-bhplayground-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
