{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Machine Learning (6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error surface lies in a space with horizontal axis for each weight and one vertical axis for the error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whole batch learning is not useful when error surface is a quadratic bowl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Too big weights result in oscillation\n",
    "- Want to move quickly with small but consistent gradient\n",
    "- Move slowly with big but inconsistent gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If dataset is highly redundant, gradient on first half is almost identical on the second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-batches are usually better than online\n",
    "- Less computation is used (compared with online)\n",
    "- Computing gradient simultaneously, can be efficient especially on GPU\n",
    "- Mini-batches need to be balanced for classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two types of learning algorithm:\n",
    "- Full gradient: non-linear conjugate gradient\n",
    "- Large NN: use mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic mini-batch gradient descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Guess an initial learning rate\n",
    "- Adjust learning rate based on convergence\n",
    "- Turn down learning rate towards end of mini-batch (when error stops decresing) (Use error on seperate validation set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Bag of Tricks for Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initializing the weights:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initial weights with small random value\n",
    "- Initialize weights to be sqrt(fan-in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shifting the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make the input vector have zero mean over the whole training set, make the error surface a nice circle\n",
    "- The hyberbolic tangent (2*logistic-1) produces hidden activations that are roughly zero mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scaling the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform so that each input has unit variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### More thorough method: decorrelate the input components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reasonable method is PCA:\n",
    " - drop PCA with smallest eigenvalues\n",
    " - Divide remaining principle components by square roots of their eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Common problems occur in multilayer networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Too big learning rate -> too big positive or negative weights\n",
    "2. The best strategy for network when using squared error or cross-entropy error is to make output equal to the proportion it should 1 (it's a local minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Becareful about turning down learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't turn down learning rate too soon or too much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Four ways to speed up mini-batch learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use \"momentum\" -> use gradient to change the velocity\n",
    "2. Use seperate adaptive learning rates for each parameter -> slowly adjust the rate using the consistency of the gradient for that parameter\n",
    "3. rmsprop -> divide the learning rate for a weight by a running average of the magnitudes of recent gradients for that weights (a mini-batch version of just using sign of the gradient)\n",
    "4. Use curvature information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Momentum Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It damps oscillations in direction of high curvature by combing gradients with opposite signs\n",
    "- It builds up spped in directions with a gentle but consistent gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The equations of momentum method\n",
    "$v(t)=\\alpha v(t-1)-\\epsilon \\frac{\\partial{E}}{\\partial{w}}(t)$  \n",
    "$\\Delta w(t)=v(t)=\\alpha v(t-1)-\\epsilon \\frac{\\partial{E}}{\\partial{w}}(t)=\\alpha \\Delta w(t-1)-\\epsilon \\frac{\\partial{E}}{\\partial{w}}(t)$  \n",
    "the weight change equal to current velocity, or previous weight change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The behavior of momentum method:\n",
    "- On tilted plane: $v(\\infty)\\frac{1}{1-\\alpha}(\\epsilon\\frac{\\partial{E}}{\\partial{w}})$\n",
    "- Beginning: may be very large gradients (it pays to use small momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use small learning rate with large momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Better type of momentum\n",
    "- standard: compute gradient at current location then take a big jump at in direction of accumulated gradient\n",
    "- Better way:\n",
    " 1. Make big jump in direction of previous accumulated gradient\n",
    " 2. Measure gradient, make a correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics\\6-3-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### A Seperate, Adaptive Learning Rate for Each Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The intuition behind seperate adaptive learning rates\n",
    "- In multilayer net, appropriate rates can vary between weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Determine individual learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Delta w_{ij}=-\\epsilon g_{ij}\\frac{\\partial{E}}{\\partial{w_{ij}}}$  \n",
    "if $(\\frac{\\partial{E}}{\\partial{w_{ij}}}(t)\\frac{\\partial{E}}{\\partial{w_{ij}}}(t-1))>0$  \n",
    "then $g_{ij}(t)=g_{ij}(t-1)+.05$  else $g_{ij}(t)=g_{ij}(t-1)*.95$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ensure big gains decay rapidly when oscillations start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gain will hover around 1 when gradients are totally random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tricks for making adaptive learning rates work better\n",
    "- limit gains to lie in some reasonable range\n",
    "- use full batch learning or very big mini-batches\n",
    "- adaptive learning rates can be combined with momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rmsprop: Divide the Gradient by a Running Average of Its Recent Magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### rprop: using only the sign of the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can escape plateauus with tiny gradients quickly\n",
    "rprop: using the sign of the gradient with step size seperately for each weight\n",
    "- increase the step size for a weight multiplicatively if sign of its last two gradients agree\n",
    "- otherwise decrease the step size multiplicatively\n",
    "- limit step sizes to be less than 50 and more than a millionth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why rprop does not work with mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It violate central idea between stochastic gradient descent  \n",
    "Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics\\6-5-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### rmsprop: A mini-batch version of rprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics\\6-5-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Further development of rmsprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combine rmsprop with standard momentum\n",
    "- Combine rmsprop with Nesterov meoentum\n",
    "- Combine rmsprop with adaptive learning rates for each connect\n",
    "- Other methods related to rmsprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Small datasets (e.g. 10,000 cases) or bigger datasets without much redundancy, use a full-batch method\n",
    " 1. Conjugate gradient\n",
    " 2. Adaptive learning rates\n",
    "- Big, redundant datasets\n",
    " 1. Gradient with momentum\n",
    " 2. rmsprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:bhplayground]",
   "language": "python",
   "name": "conda-env-bhplayground-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
