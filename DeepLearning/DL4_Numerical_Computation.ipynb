{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap4: Numerical Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Underflow: numbers near zero are rounded to zero\n",
    "- Overflow: numbers with large magnitude are approximated as $\\infty$ or $-\\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conditioning**: how rapidly a function changes with respect to small changes -> sensitive to previous errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steepest descent**: $x'=x-\\epsilon \\nabla_xf(x)$ -> $\\epsilon$ is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jacobian matrix**: matrix containing all partial derivative of function whose inut and ouput are both vectors  \n",
    "function $\\mathbf{f}:\\mathbb{R}^m\\rightarrow\\mathbb{R}^n$, Jacobian matrix: $\\mathbf{J}\\in\\mathbb{R}^{n*m}, J_{i,j}=\\frac{\\partial}{\\partial x_j}f(x)_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hessian matrix**: Jacobian of the gradient  \n",
    "$H(f)(x)_{i,j}=\\frac{\\partial^2}{\\partial x_i\\partial x_j}f(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local minimum: $f'(x)=0$ and $f''(x)>0$, inconclusive point: $f''(x)=0$  \n",
    "multi-dimensional: $\\nabla_xf(x)=0$ and Hessian is positive definite, inconclusive point: non-zero eigenvalues of Hessian have the same sign but at least one eigenvalue is zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First-order optimization algorithms: gradient descent\n",
    "- Second-order optimization algorithms: Newton's method, Hessian matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lipschitz continuous: function $f$ whose rate of change is bounded by a Lipschitz constant $\\mathcal{L}$:  \n",
    "$\\forall x, \\forall y, |f(x)-f(y)|\\le\\mathcal{L}||x-y||_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Karush-Kuhn-Tucker(KKT)** -> **generalized Lagrange function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbb{S}=\\{x|\\forall i, g^{(i)}(x)=0 \\ and \\ \\forall j, h^{(j)}(x)\\le 0\\}$  \n",
    "$\\mathcal{L}(x,\\lambda,\\alpha)=f(x)+\\sum_i\\lambda_ig^{(i)}(x)+\\sum_j\\alpha_jh^{(j)}(x)$ <- Lagrangian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$min_x max_\\lambda max_{\\alpha,\\alpha\\ge0}\\mathbb{L}(x,\\lambda,\\alpha)$ equals to $min_{x\\in \\mathbb{S}}f(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- primal feasibility: $g_i(x^*)\\le 0, \\ i=1,\\dots,m \\ and \\ h_i(x^*)=0, \\ i=1,\\dots,p$\n",
    "- dual feasibility: $\\alpha_i^*\\ge0, i=1,\\dots,m$\n",
    "- complementary slackness: $\\alpha_i^*g_i(x^*)=0,i=1,\\dots,m$\n",
    "- lagrangian stationary: $\\nabla_x\\mathcal{L}(x^*,\\alpha^*,\\beta^*)=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:bhplayground]",
   "language": "python",
   "name": "conda-env-bhplayground-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
