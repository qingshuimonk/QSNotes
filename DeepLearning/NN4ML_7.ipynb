{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Machine Learning (7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Sequences: A Brief Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Getting targets when modeling sequences:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transform sequence into another sequence\n",
    "- Predict next input in sequence\n",
    "- When predic next term, it blurs distinction between supervised learning and unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Memoryless models for sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/7-1-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Beyond memoryless models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sequence: have a model with hidden state -> internal dynamics\n",
    "- can store information in hidden state for long time\n",
    "- Noisy dynamics -> noisy hidden state\n",
    "- Infer a probability distribution over space of hidden state vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference is only tractable for two types of hidden state model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Linear dynamical system:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics\\7-1-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hidden Markov Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics\\7-1-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Limitation of HMMs:\n",
    "- total information of hidden states: log(N)\n",
    "- utterance example: HMM needs $2^{100}$ states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear dynamics systems and HMM are stochastic models\n",
    " 1. But posterior is a deterministic function\n",
    "- RNNs are deterministic\n",
    " 1. Think hidden state of RNN equivalent to deterministic probability distribution of stochastic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What RNNs can exhibit:\n",
    "- They can oscillate\n",
    "- They can settle to point attractors\n",
    "- Behave chaotically\n",
    "- Could learn little programs capture nugget knowledge and run parallel\n",
    "- But RNN is very hard to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training RNNs with Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### equivalence between feedforward nets and recurrent nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics\\7-2-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Backpropagation with weight constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradients as usual, then modify to satisfy the constraints:\n",
    "To constrain: $w_1=w_2$  \n",
    "We need: $\\Delta w_1=\\Delta w_2$  \n",
    "Compute: $\\frac{\\partial{E}}{\\partial{w_1}}$ and $\\frac{\\partial{E}}{\\partial{w_2}}$  \n",
    "Use $\\frac{\\partial{E}}{\\partial{w_1}}+\\frac{\\partial{E}}{\\partial{w_2}}$ for $w_1$ and $w_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Backpropagation through time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. RNN can be regard as layered, feed-forward net with shared weights\n",
    "2. The forward pass builds up stack activities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treat initial state as a parameter to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Providing input to RNN\n",
    "1. specify initial states for all units\n",
    "2. specify initial states for subset of units\n",
    "3. sepcify same subset of units at every time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. specify targets for desired final states\n",
    "2. specify targets for last few steps\n",
    " - good for learning tractors\n",
    " - esay to add in extra error derivatives\n",
    "3. Specify desired activity of a subset of units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Toy Example of Training an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding up to binary numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![](pics/7-4-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why it is Difficult to Train an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. use squashing function like logistic prevent activity vectors from exploding\n",
    "2. backward pass is completely linear -> doulbe error derivatives at the final layer, all error derivatives will double\n",
    "3. Many layers -> big/small weights -> gradients explode/shrink exponentially -> RNN with long sequence will have this problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Why back-propagated gradient blows up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/7-4-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Four effective ways to learn an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Long short term memory\n",
    "2. Hessian free optimization\n",
    "3. Echo state network -> echo $\\sim$ memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Implement a memory cell in NN\n",
    "- use circuit that implements an analog memory cell\n",
    "- 'keep'/'write'/'read' gate\n",
    "- we can backpropagate because logstic cells have nice derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### An example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/7-5-1.png)\n",
    "![](pics/7-5-2.png)\n",
    "![](pics/7-5-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Reading cursive handwritting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input (x,y,p) -> p: pen is up or down  \n",
    "output: sequence of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:bhplayground]",
   "language": "python",
   "name": "conda-env-bhplayground-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
