{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "* [Introduction to Probabilistic Topic Models](http://menome.com/wp/wp-content/uploads/2014/12/Blei2011.pdf)\n",
    "* [Empirical Study of Topic Modeling in Twitter](http://snap.stanford.edu/soma2010/papers/soma2010_12.pdf)\n",
    "## Latent Dirichlet Allocation (LDA)\n",
    "![Topic Modeling Demo Figure](http://www.scottbot.net/HIAL/wp-content/uploads/2011/11/IntroToLDA.png)\n",
    "### Definitions\n",
    "We define a **topic** to be a distribution over a fixed vocabulary. Each document will be processed in this two-stage process:\n",
    "1. Randomly choose a distribution over topics\n",
    "2. For each word in document\n",
    "    * Randomly choose a topic from distribution over topics\n",
    "    * Randomly choose a word from the corresponding distribution over the vocabulary\n",
    "Define some notations below:\n",
    "* topics $\\beta_{1:K}$, where each $\\beta_{1:K}$ is a distribution over vacabulary (left blocks in figure)\n",
    "* topic proportions of dth document $\\theta_d$, and $\\theta_{d,k}$ is proportion for topic k in document d\n",
    "* topic assignment for document d: $z_{d}$, and $z_{d,n}$ is topic assignment for nth word in document d\n",
    "* observed nth word in document d: $w_{d,n}$\n",
    "### Probabilistic model of LDA:  \n",
    "$p(\\beta_{1:K},\\theta_{1:D},z_{1,D},w_{1,D})=\\prod_{i=1}^{K}p(\\beta_i)\\prod_{d=1}^{D}p(\\theta_d)\\big(\\sum_{n=1}^{N}p(z_{d,n}|\\theta_d)p(w_{d,n}|\\beta_{1:K},z_{d,n})\\big)$  \n",
    "The graphical model is shown below:  \n",
    "![Topic Modeling Graphical Model](https://filebox.ece.vt.edu/~s14ece6504/projects/alfadda_topic/main_figure_3.png)\n",
    "### Posterior computation\n",
    "$p(\\beta_{1:K},\\theta_{1:D},z_{1:D}|w_{1:D})=\\frac{p(\\beta_{1:K},\\theta_{1:D},w_{1:D})}{p(w_{1:D})}$\n",
    "## Topic Modeling Schemes\n",
    "### MSG\n",
    "1. Train LDA on **all training messages**\n",
    "2. Aggreage training messages from the **same user**\n",
    "3. Aggregate test messages by **same user**\n",
    "4. Use the trained model to infer topic mixtures of each testing message\n",
    "### USER\n",
    "1. Train LDA on **aggregated user profiles**\n",
    "2. Aggregate testing messages from **same user**\n",
    "3. Use the trained model to infer topic mixtures of each testing message\n",
    "### TERM\n",
    "1. For each term in training set, aggregate messages with that term\n",
    "2. train LDA on **training term profiles**\n",
    "3. Build user profiles in training and testing set respectively\n",
    "4. Use the trained model to infer topic mixtures of each testing message\n",
    "## Measure Similarity of Topics\n",
    "Jensen-Shannon divergence:\n",
    "$D_{JS}=\\frac{1}{2}D_{KL}(P||R)+\\frac{1}{2}D_{KL}(Q||R)\\\\R=\\frac{1}{2}(P+Q)$  \n",
    "where $D_{KL}(P||R)=\\sum_{n=1}^{M}\\beta_{P,n}\\log\\frac{\\beta_{P,n}}{\\beta_{R,n}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:bhplayground]",
   "language": "python",
   "name": "conda-env-bhplayground-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
