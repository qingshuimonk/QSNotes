{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 39. \n",
      "    Loss 9442.34179688. Time 11.3156471252\n",
      "T-/a e                                                                                                                                                                                                                                                                                                       \n",
      "Iter 79. \n",
      "    Loss 8287.05664062. Time 9.65837717056\n",
      "TFS the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the t\n",
      "Iter 119. \n",
      "    Loss 7515.04101562. Time 9.55453896523\n",
      "The the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere \n",
      "Iter 159. \n",
      "    Loss 6959.69921875. Time 8.67818903923\n",
      "The the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere \n",
      "Iter 199. \n",
      "    Loss 6399.76464844. Time 8.63931512833\n",
      "The the sere the serection the serection the serection the serection the serection the serection the serection the serection the serection the serection the serection the serection the serection the serection the serection the serection the serection the serection the serection the serection the sere\n",
      "Iter 239. \n",
      "    Loss 6154.20214844. Time 8.50697898865\n",
      "The and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and a\n",
      "Iter 279. \n",
      "    Loss 5918.70166016. Time 8.29506897926\n",
      "The ard the reprosent and the reare pored converent and the reare pored converent and the reare pored converent and the reare pored converent and the reare pored converent and the reare pored converent and the reare pored converent and the reare pored converent and the reare pored converent and the r\n",
      "Iter 319. \n",
      "    Loss 6129.37011719. Time 8.46809697151\n",
      "The ard the convex on a proviment and the convex on a proviment and the convex on a proviment and the convex on a proviment and the convex on a proviment and the convex on a proviment and the convex on a proviment of the erally in the erally in the erally in a providen in the erally in a providen in \n",
      "Iter 359. \n",
      "    Loss 5417.99853516. Time 8.2405500412\n",
      "The and the seating the reconting the propose a propose a propose a propose a propose a propose a propose a propose a propose a propose a propose a propose a propose a propose a propose a propose a propose a propose a propose a propose a propose a propose a propose a propose a propose a propose a pro\n",
      "Iter 399. \n",
      "    Loss 5281.10888672. Time 8.23715305328\n",
      "The and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the n\n",
      "Iter 439. \n",
      "    Loss 5056.15527344. Time 8.1899869442\n",
      "The and the convex on the convex on the convex on the convex on the convex on the convex on the convex on the convex on the convex on the convex on the convex on the convex on the convex on the convex on the convex on the convex on the convex on the convex on the convex on the convex on the convex on\n",
      "Iter 479. \n",
      "    Loss 4403.64453125. Time 8.19066405296\n",
      "The a proposed the such as the expening the expening the expening the expening the expening the expening the expening the expening the expening the expening the expening the expening the expening the expening the expening the expening the expening the expening the expening the expening the expening t\n",
      "Iter 519. \n",
      "    Loss 4684.18066406. Time 8.21730184555\n",
      "The and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the n\n",
      "Iter 559. \n",
      "    Loss 5388.80761719. Time 8.20976495743\n",
      "These the successfiling and experiments of the and the successfiling and experiments of the and the successfiling and experiments of the and the successfiling and experiments of the and the successfiling and experiments of the and the successfiling and experiments of the and the successfiling and exp\n",
      "Iter 599. \n",
      "    Loss 4476.16064453. Time 9.44792199135\n",
      "The proposed by the network architectures that the network architectures that the network architectures that the network architectures that the network architectures that the network architectures that the network architectures that the network architectures that the network architectures that the ne\n",
      "Iter 639. \n",
      "    Loss 3836.76245117. Time 10.6421580315\n",
      "The convergence rates of the convex optimization of the convex optimization of the convex optimization of the convex optimization of the convex optimization of the convex optimization of the convex optimization of the convex optimization of the convex optimization of the convex optimization of the co\n",
      "Iter 679. \n",
      "    Loss 4100.41113281. Time 9.6290268898\n",
      "The proposed models and the representation to a state-of-the-art matrix state-of-the-art matrix state-of-the-art matrix state-of-the-art matrix state-of-the-art matrix state-of-the-art matrix state-of-the-art matrix state-of-the-art matrix state-of-the-art matrix state-of-the-art matrix state-of-the-\n",
      "Iter 719. \n",
      "    Loss 3621.66308594. Time 10.045566082\n",
      "The processing and computation of the activity of deep neural networks (ion of the activity of deep neural networks (inear a problems and information are iteration of the architectures and recognition to a serter for the information are computation of the activity of deep neural networks (ion of the \n",
      "Iter 759. \n",
      "    Loss 4005.00488281. Time 8.74469208717\n",
      "The approach to a simple of the are an intion that in a simple of the are an intion that in a simple of the are an intion that in a simple of the are an intion that in a simple of the are an intion that in a simple of the are an intion that in a simple of the are an intion that in a simple of the are\n",
      "Iter 799. \n",
      "    Loss 3996.64794922. Time 8.71660995483\n",
      "The existing the simel of the simel of the simel of the simel of the simel of the simel of the simel of the simel of the simel of the simel of the simel of the simel of the simel of the simel of the simel of the simel of the simel of the simel of the simel of the simel of the simel of the simel in th\n",
      "Iter 839. \n",
      "    Loss 3545.96435547. Time 8.28691315651\n",
      "The conventional neural networks are computation of the network depporent deep neural networks are computation of the network depporent deep neural networks are computation of the network depporent deep neural networks are computation of the network depporent deep neural networks are computation of t\n",
      "Iter 879. \n",
      "    Loss 3434.36230469. Time 8.3907930851\n",
      "The framework by the and maximize the approach for the training and state-of-the-art networks (DNN) have a convex optimization are non convex to train and the model by the activation from the training and state-of-the-art networks (DNN) have a convex optimization are non convex to train and the model\n",
      "Iter 919. \n",
      "    Loss 3429.43017578. Time 9.23493504524\n",
      "The experimental results on the function to the experiments the experimental results show dopon the successfully the experimental results on the function to the experiments the experimental results show dopon the successfully the experimental results on the function to the experiments the experimenta\n",
      "Iter 959. \n",
      "    Loss 3581.93334961. Time 9.83131194115\n",
      "The for the sumporminate probability of the model accuracy of the network problems of the network provide a deep neural networks are solecrian problem in the sume of the network provide a deep neural networks are solecrian problem in the sume of the network provide a deep neural networks are solecria\n",
      "Iter 999. \n",
      "    Loss 3243.95336914. Time 9.45930886269\n",
      "The proposed models (RNN) and the layers of complex to achieved and the onderstors that can be trained on the layers of complex to and state-of-the-art models on a content of the layers of complex to and state-of-the-art models on a content of the layers of complex to and state-of-the-art models on a\n",
      "Iter 1039. \n",
      "    Loss 3541.8137207. Time 8.87307620049\n",
      "The proposed models and in the network are not of the network are not of the network are not of the network are not of the network are not of the network are not of the network are not of the network are not of the network are not of the network are not of the network are not of the network are not o\n",
      "Iter 1079. \n",
      "    Loss 3163.79833984. Time 9.077272892\n",
      "The approach to a large straiget, a formation of the activation from the improvement the activations in a single learning algorithms to a simple of the activation from the improvement the activation for the information estimate of the activation from the improvement the activation for the information\n",
      "Iter 1119. \n",
      "    Loss 3412.48632812. Time 9.41637587547\n",
      "The existing the state-of-the-art model by a simple and the state-of-the-art model by a simple and the state-of-the-art model by a simple and the state-of-the-art model by a simple and the state-of-the-art model by a simple and the state-of-the-art model by a simple and the state-of-the-art model by \n",
      "Iter 1159. \n",
      "    Loss 2927.78808594. Time 9.80268216133\n",
      "The preservine a gradient descent deep networks are state-of-the-art network architectures state-of-the-art method for the computation of the context of the context of the context of the context of the context of the context of the context of the context of the context of the context of the context o\n",
      "Iter 1199. \n",
      "    Loss 3555.26171875. Time 8.9773349762\n",
      "The framework for each layer of the network depprion architectures for each layer of the network depprion architectures for each layer of the network depprion architectures for each layer of the network depprion architectures for each layer of the network depprion architectures for each layer of the \n",
      "Iter 1239. \n",
      "    Loss 3302.72949219. Time 9.37932610512\n",
      "The approach is network architectures by a similer but previous receating recegr networks and related models for a generalized processing experiments of a new a connection we show that the a training results shape the architecture of a new a connection we show that the a training results shape the ar\n",
      "Iter 1279. \n",
      "    Loss 3258.20410156. Time 8.78642201424\n",
      "The accuracy network computational neural networks are speedup sith and the network proposed approx mathines the network proposed and the network proposed approx mathines the network proposed and the network proposed approx mathines the network proposed and the network proposed approx mathines the ne\n",
      "Iter 1319. \n",
      "    Loss 3218.96337891. Time 9.39284396172\n",
      "The approaches of the layers of the layers of the layers of the layers of the layers of the layers of the layers of the layers of the layers of the layers of the layers of the layers of the layers of the layers of the layers of the layers of the layers of the layers of the layers of the layers of the\n",
      "Iter 1359. \n",
      "    Loss 3008.58496094. Time 9.41062116623\n",
      "The approach the network are a single layers to the state-of-the-art for the state-of-the-art for the speed pooling of the network are a single layers to the state-of-the-art for the state-of-the-art for the speed pooling of the network are a single layers to the state-of-the-art for the state-of-the\n",
      "Iter 1399. \n",
      "    Loss 3177.10205078. Time 9.23207497597\n",
      "The approach to the existing and the convergence to a feature as a betcent data that have the approach to orher term to se the empirical y existing and the convergence rates of the network depind to provide a stationary points with network computational results recognition in the convergence rates of\n",
      "Iter 1439. \n",
      "    Loss 3284.36279297. Time 9.12303900719\n",
      "The approach for the state-of-the-art models can be used to train a similar state-of-the-art models (e.g., results on the to train a similar stace) and an ASR tasks classification accuracy results in this paper on a similar and layers and providing and to the signal computational optimization structu\n",
      "Iter 1479. \n",
      "    Loss 3127.89013672. Time 9.5849571228\n",
      "The proposed model and layer of the network all wey have a similar escelly prediction be trained on a deep neural networks (DNNs to the simple model atation in the network all wey a simple and a feedforward neural networks (DNNs to the simple model atation in the network all wey a simple and a feedfo\n",
      "Iter 1519. \n",
      "    Loss 3135.47753906. Time 9.17929911613\n",
      "The proposed method is to be deep networks and introduce a new aralyse to train and the layers of recurrent units of pool size. We also explodit pooling these the difficult to provide information are itterpided in the data are able to better under to be defensive difficult to previous different despr\n",
      "Iter 1559. \n",
      "    Loss 2845.53222656. Time 9.12389588356\n",
      "The art neural network (DNN) to the system to form the sementation of the network are standard approach is within which to separate of the architecture of deep neural network trains a makes the sumery of stochastic gradient descent the search of the network are some training of the network well out a\n",
      "Iter 1599. \n",
      "    Loss 3048.94824219. Time 8.66003108025\n",
      "The matrix can be computational and the network and manifold $\\lamed on a deep neural networks (DNN) have a constrained by the network and manifold $\\lamed on a deep neural networks (DNN) have a constrained by the network and manifold $\\lamed on a deep neural networks (DNN) have a constrained by the \n",
      "Iter 1639. \n",
      "    Loss 2744.07739258. Time 8.13857698441\n",
      "The fact that a new the contrastive different state-of-the-art methods for the contral for the contrastive different state-of-the-art methods for the contral for the contrastive different state-of-the-art methods for the contral for the contrastive different state-of-the-art methods for the contral f\n",
      "Iter 1679. \n",
      "    Loss 2860.69750977. Time 8.40955305099\n",
      "The experiments leveral problems with a fee from stochastic gradient descent that can be training deep neural networks that can be trained of a generalization processing of the state-of-the-art designivged with desprystions that can be trained on a groups from computation in the state-of-the-art desi\n",
      "Iter 1719. \n",
      "    Loss 2702.65283203. Time 8.96770715714\n",
      "The algorithm is to be reduled to previous d preservition of the network depth and layers to empirically introduce a new premise the training of the network depth and layers to empirically introduce a new premise the training of the network depth and layers to empirically introduce a new premise the \n",
      "Iter 1759. \n",
      "    Loss 2958.61621094. Time 8.09191703796\n",
      "The existing prediction methods for standard RNN with the encoder and stacking methods of the layer of the art on simels on the each layer of the art on simel speed output function to train methods of the RNN with the encoder and scales with a simile constrace of the application methods for standard \n",
      "Iter 1799. \n",
      "    Loss 2881.75268555. Time 8.12740302086\n",
      "The fully-everobals the method learned generators as the method for the training settings of the further log-likerialion weights are sumplestry to inference in state-of-the-art feedforward neural networks (DNNs) as the method for the training settings of the further log-likerialion weights are sumple\n",
      "Iter 1839. \n",
      "    Loss 2633.79199219. Time 8.08329105377\n",
      "The further in a deep neural network demonstrate that are able to achieve an applying in a deep neural network demonstrate that are able to achieve an applying in a deep neural network demonstrate that are able to achieve an applying in a deep neural network demonstrate that are able to achieve an ap\n",
      "Iter 1879. \n",
      "    Loss 2884.48681641. Time 8.49703907967\n",
      "The approach to the expensive to train machine learning rate algorithm to be reduced as the approach to a search of a simple and the different to the compection of the network demonstrated the embeddings of the successfold that the proposed RNN models are gradient in the accurate capacity of deep neu\n",
      "Iter 1919. \n",
      "    Loss 2774.28588867. Time 8.22370696068\n",
      "The approach is computation in the recontiouls normalization procedsing a deep neural networks as a precision for deep neural networks (DNNs) and have a convex optimization procedures that have been well as a batch normal structures. The state-of-the-art pre-training of the network and convergence ra\n",
      "Iter 1959. \n",
      "    Loss 2835.28930664. Time 8.17501616478\n",
      "The frameworks in the context of pooling operations is understandarch can astically istention of the information of the information of the information of the representation learning approach for training pooling operations with the convergence of the information of the representation learning approac\n",
      "Iter 1999. \n",
      "    Loss 2806.75415039. Time 8.09608602524\n",
      "The approach to a search for a generalized RNNs classification benchmark datasets. For anoutely and maxout unit provide an approach to a carters and features for the ore-training descent that this algorithm for each learning algorithm for standard learning algorithms, the proposed $L_p$ unit on the p\n",
      "Iter 2039. \n",
      "    Loss 2800.41479492. Time 8.07550001144\n",
      "The approach is computation estimated of computational computational an layer convolutional neural networks to respect to a deep network architectures by and in propering and an addrying deep neural networks to results such as the method of the network depend on the method of the network depend on de\n",
      "Iter 2079. \n",
      "    Loss 2851.09619141. Time 8.33832907677\n",
      "The approach constrained by the accuracy resulve the approach is compared to a large machine learning tasks on complex signals on MBN and in an augmented structure of the non-convex optimization are effectiveness of MBN in a distribution in the context of possion to train model for the lon-linear net\n",
      "Iter 2119. \n",
      "    Loss 2699.02832031. Time 8.15232801437\n",
      "The approach for data that connection structuraters. The algorithm for success of the subject is estimate the state-of-the-art performance of an autoencoder and structure, such as a which model accuracy. We formalize the transforms with state-of-the-art results on a connection with the structure of r\n",
      "Iter 2159. \n",
      "    Loss 2690.96142578. Time 8.11022496223\n",
      "The convex optimization processor computation of the network depth and iterative proposed methods of the network depth and iterative proposed methods of the network depth and iterative proposed methods of the network depth and iterative proposed methods of the network depth and iterative proposed met\n",
      "Iter 2199. \n",
      "    Loss 2720.6953125. Time 8.08835911751\n",
      "The approach for each can be speed opting. This a simple model parameter valuance of the DNNs in a distributed to speed up the competitive with a classification benefits that can be communication learning tasks. However, model relatively be to accerarg of the sound representation complexity of MBN is\n",
      "Iter 2239. \n",
      "    Loss 2988.68774414. Time 8.12226200104\n",
      "The accuracy recognition transforms to a context deep learning algorithms to a corressonsing of the network are learning rate models as well as providing recent recognition transforms to model to a single methods for superior to preserves pre-training methods, and compressition with the input data su\n",
      "Iter 2279. \n",
      "    Loss 2561.7487793. Time 8.12003684044\n",
      "The approach for the large-scale deep neural networks is an initial results such neurons for training (aighes) to functions of the sequence training. We demonstrate the sembirionally information and results such neurons for the large-scale information of the data depond network is and the recent intr\n",
      "Iter 2319. \n",
      "    Loss 2663.40576172. Time 8.07225513458\n",
      "The approach is a subset on the effectiveness of an embedding the artary some network as a benchmark dor distribution of a new regularization accurate model as a beherable provide a generating method for the training deep neural network (RNN) architectures are model as a between the network depth and\n",
      "Iter 2359. \n",
      "    Loss 2584.46386719. Time 8.10131692886\n",
      "The proposed approach is computational open the model that higher on the network architectures are additiality propagation and an architecture that have the proposed deep neural networks are able to a single log-likelihian promist in parally with a simple and an each notcorved on the proposed approac\n",
      "Iter 2399. \n",
      "    Loss 2688.70849609. Time 8.05409908295\n",
      "The basks strective model for the indinited domain the activation function that it context and stacked RNNs on a novel standard and the convergence of the solution for the inference of the interpreted and shown to standard Convolutional neural networks are simple and the context of pool based on the \n",
      "Iter 2439. \n",
      "    Loss 2673.20556641. Time 8.07063794136\n",
      "The approach to learn non-linear networks (DNNs) as well as process of state-of-the-art results on the state-of-the-art results on the secting inference in state-of-the-art results on the secting inference in state-of-the-art results on the secting inference in state-of-the-art results on the secting\n",
      "Iter 2479. \n",
      "    Loss 2464.24560547. Time 8.1268260479\n",
      "The proposed method is the constructileal networks (DNN) sourcul of the network depends on the method of a standar approaches convolutional layers, such as the structure of semain be communated do input-sutmodifitt that our approach inference tasks. The reconstruction architectures by inveltes that i\n",
      "Iter 2519. \n",
      "    Loss 2291.49414062. Time 8.42068982124\n",
      "The RFN learning algorithm is essembile high-deep a new layers. Monel cyntem non-linear and massively convexity of MBN but also analysis of a sel cestoul results shaped to strace of such architecture maxout unit problems on CTCCi--CIFAR-10, mole deep networks on a single log in the lon-linear, (i) de\n",
      "Iter 2559. \n",
      "    Loss 2380.09228516. Time 8.35505986214\n",
      "The proposed models and factors for each paremal which tepprous of maxout unit accuser deep networks. Thes in over search for a group of the desirations for systematic we show that the functional pooling operators ser atcemtigation with a single layer of the network are layer at the network and deep \n",
      "Iter 2599. \n",
      "    Loss 2517.0793457. Time 8.27218389511\n",
      "The approach for the input for learning improvemets is a parallelized to one of the input speech recognition into possible training neural networks are state-of-the-art setting, on a stationark with state-of-the-art neural networks to such deep learning algorithms is an incuraly yneved applications w\n",
      "Iter 2639. \n",
      "    Loss 2333.02978516. Time 8.23826313019\n",
      "The proposed RNN ather optimization integented by the activation extonsive to the approach for each painible crantsmizants perceptron of an extension and capablearning algorithms in the recently interpretation on a large number of regularizer network (RNN) architectures of a new recognition system co\n",
      "Iter 2679. \n",
      "    Loss 2444.43554688. Time 9.54670810699\n",
      "The approach is the network. The proposed and the recurrent neuronal computation of a sequence of the input of the network. As a stationary pooling operation of a stati-------------------------------------------------------------------------------------------------------------------------------------\n",
      "Iter 2719. \n",
      "    Loss 2389.71582031. Time 9.02655482292\n",
      "The problems and the convergence rates for the convergence rates for the convergence rates for the convergence rates for the convergence rates for the convergence rates for the convergence rates for the convergence rates for the convergence rates for the convergence rates for the convergence rates fo\n",
      "Iter 2759. \n",
      "    Loss 2306.94921875. Time 8.20735001564\n",
      "The approach computational analysis the state-of-the-art recurrent neural networks (DNN) with distillation into a single model and maxout to the state-of-the-art results on the massively parameter structure, stacking multiple defensive distillation methods to the state-of-the-art decame exts learned \n",
      "Iter 2799. \n",
      "    Loss 2183.37158203. Time 8.1530148983\n",
      "The proposed method is network content approach in proposed method is network content approach from neural networks to recent apprimase size provide a pointan between the deep learning architectures by a single layers, and the parameters to computation. In this paper we propose a novel pre-training m\n",
      "Iter 2839. \n",
      "    Loss 2611.1328125. Time 8.0962600708\n",
      "The RNN models in the context of pool nonlinear himanked especiall learning and image classification), ever intains, show that our neural network for deep neural networks are simple constraints and sticat on CIFAR-10 (7.51%), CIFAR-100 handwriting results on a large classes a new by directly extract \n",
      "Iter 2879. \n",
      "    Loss 2383.95800781. Time 8.07761287689\n",
      "The proposed maxout unit (RNN) dependents and show that further model as a few state-of-the-art methods of the maximems of predictions (e.g., restricted of a set of units in the state-of-the-art on the maxout unit (GASF-surcesting to the input of the approach for deep neural networks (DNNs) as well a\n",
      "Iter 2919. \n",
      "    Loss 2428.81640625. Time 8.1967561245\n",
      "The proposed deep neural networks are single neural networks to postening important for exact tasks of accuracy. We also deverop on schem, with no layers to reduce the component architectures by invariance matomatcomparallelism, achieved we propose a convex optimization, as well as a between the deep\n",
      "Iter 2959. \n",
      "    Loss 2367.03979492. Time 8.17996788025\n",
      "The proposed GF-Res, we propose a new adaptive the speedup on the approach for each pait models and parallel speedup different RNN using any the network we speedints in the empirical insights includicas an embersiver generalization error in the dataser of each local methods to the input shapping buta\n",
      "Iter 2999. \n",
      "    Loss 1954.35766602. Time 8.09332704544\n",
      "The recurrent units in the state-of-the-art on the main remains a group out theoretical results such as sems as weight that the proposed model in the state-of-the-art on the main remains a group output of the functions for training deep neural networks. These test the proposed model as the model in t\n",
      "Iter 3039. \n",
      "    Loss 2473.49047852. Time 8.13432192802\n",
      "The problem dimension and a standard deep neural networks with the context and DNN training is computation introduces an aut tiee convergence of the activation functions that regularization techniques and stacked RNNs on a deep learning algorithms such as stochastic gradient descent that can be reduc\n",
      "Iter 3079. \n",
      "    Loss 2373.19384766. Time 8.22064089775\n",
      "The proposed RNN and model accuracy of the input saggesting toat be able to the output of the application by a simple models in derivation for the optimization proceds can be used as a computation with a connection with the that the prediction with the state-of-the-art on complex to train the predict\n",
      "Iter 3119. \n",
      "    Loss 2291.0065918. Time 9.78527998924\n",
      "The proposed models that learning algorithms to train a sturial taptuse the generative powerful neural network (DNN) acoustic framework and the network architectures by a staling optimization of a sequence of such learning algorithms to recognition in the state of the art on speech recognition in the\n",
      "Iter 3159. \n",
      "    Loss 2487.9453125. Time 9.19079303741\n",
      "The proposed RNN wo show that our previous CIFAR-100 and non-convex optimization of a new activation function of the neural net of difficult, on seperate accuracely exploding method massive to the simple state-of-the-art features, why backpropagation are in a random which establishes a light in a lar\n",
      "Iter 3199. \n",
      "    Loss 2339.61083984. Time 8.09101390839\n",
      "The back-propagation can be combined by the approach for gradients. We introduce a new synchronization processons (to learn networks. HF understoods of the RNN models in the same training deep neural networks (DNNs) as the state-of-the-art on complex incorporation framework can be max-pooling, each n\n",
      "Iter 3239. \n",
      "    Loss 2242.28222656. Time 8.10532999039\n",
      "The proposed deep neural networks. We demonstrate how this network proposed test that is infeas, RNNs have been the generatoms for deep neural networks. We demonstrate how this network proposed test that is infeas, RNNs have been the generatoms for deep neural networks. We demonstrate how this networ\n",
      "Iter 3279. \n",
      "    Loss 2248.00830078. Time 8.31565594673\n",
      "The proposed RFN for the effectiveness of our previous widely used in trained to seperautien indupervised by a similar output of severalization from a tore with the training problems for supervision. The proposed RFN for the effectiveness of our previous widely used in trained to seperautien induperv\n",
      "Iter 3319. \n",
      "    Loss 2438.83618164. Time 8.28281092644\n",
      "The experiments on a single layers, as well as a statialize for state-of-the-art results on which to a simple for the existing researchers of the standard RNNs can be extlimitations of the standard RNNs can be extlimitations of the standard RNNs can be extlimitations of the standard RNNs can be extli\n",
      "Iter 3359. \n",
      "    Loss 2346.7277832. Time 8.09487199783\n",
      "The benchmark datasets, phose the network depth and it is a standard function of the network depth and it is a standard function of the network depth and it is a standard function of the network depth and it is a standard function of the network depth and it is a standard function of the network dept\n",
      "Iter 3399. \n",
      "    Loss 2178.72119141. Time 8.11311101913\n",
      "The proposed RNN and provides a pooling operators that can be trained using a chollen interpretation of a new recunrent learning method is overcime be wele multaleal some prediction by learning computation performance on a low layer of the number of deep networks on a single models in the input out a\n",
      "Iter 3439. \n",
      "    Loss 2274.20166016. Time 8.11548900604\n",
      "The backpropagation and the variance reduction that is infeas typropoot feedforward neural networks (DNN) to parameters and (2) theoretical justulemization group, empirical inference that from the proposed maximal results in training effective deep learning which to is not for our preverent in trie s\n",
      "Iter 3479. \n",
      "    Loss 2152.20507812. Time 8.12765598297\n",
      "The based methods on the main learning algorithms is not features are able to access are not feed to training and dropout have been such as the sume the convergence rates of the network depth and image recurrent learning algorithms is not features are able to access are not feed to training and dropo\n",
      "Iter 3519. \n",
      "    Loss 2277.51074219. Time 8.09983086586\n",
      "The back-propagated to predict the gradient of the layers. Fow various approximation of an eatying a group ortention model accelerator chip (a. Threques, the maximum complex time. We show that the training products, which capture, but with non-convex optimization problem construct and capacity for la\n",
      "Iter 3559. \n",
      "    Loss 2242.83520508. Time 8.11704611778\n",
      "The proposed method is neural networks are able to model to adversarial samples by avoided fouts and recognition in neural networks (DNN) with minima computation explains why a pouts oned the advantage of deep neural networks (DNNs) as a standard approachescilt is a plex-ben points are not and layer-\n",
      "Iter 3599. \n",
      "    Loss 2126.80737305. Time 8.13892292976\n",
      "The proposed RFN for the convergence of the recurrent learning to an extremely of neural networks are overcoming learning can be ablied in the context of prediction processing and random acoist. We invess structured sparsity of storicon in a simple supervised frants of convexity to on inspareed to pr\n",
      "Iter 3639. \n",
      "    Loss 2250.77929688. Time 8.16745305061\n",
      "The experiments on a simple propose a new advensing essential their behavior converiane reseating or gradient works that requires set of construct deep neural network architecture that are able to a simple propose a new advensing essential their behavior converiane reseating or gradient works that re\n",
      "Iter 3679. \n",
      "    Loss 2325.58569336. Time 8.08199691772\n",
      "The backpropagation of the network depth and it is composition to the conventional layer-wise method which is not as deep network structure classification parallel size is unlabele of the input data phinengence of the network depth and it is composition to the conventional layer-wise method which is \n",
      "Iter 3719. \n",
      "    Loss 2243.2734375. Time 8.30808997154\n",
      "The back-propagated error gradient and challenging results in a clays in terms of the speedup of a clarsing and data and different distributions. Our approach is competitive with state-of-the-art model convex optimization problems with standard approach for training deep networks (DNN) have a poratio\n",
      "Iter 3759. \n",
      "    Loss 2403.7253418. Time 8.30653595924\n",
      "The effect of sequence language modeling and synthesizing and to the successfully provided a class inputs and representations as well as various deep neural networks (DNNs) as way. In contrast, we propose a novel architectures by using the recently information for recognition systems with minimal orb\n",
      "Iter 3799. \n",
      "    Loss 2164.4440918. Time 8.26095890999\n",
      "The based methods to architectures spressing the approach for deep neural networks are able to accerstopsess are induce that it reinet functions is wire architectures by interpace in standard Ress race recently provide researdizeral which that deep neural networks are able to accerstopsess are induce\n",
      "Iter 3839. \n",
      "    Loss 2149.09179688. Time 8.31095790863\n",
      "The back-propagated error relation has been unclear how to learn compared to the training methods to a butived and latent only the new a composed and transposed convergence and models and max boors. Atrecture the output of the network. As any laser of multiple different models on the tasks of muchili\n",
      "Iter 3879. \n",
      "    Loss 2210.7487793. Time 8.29364490509\n",
      "The effect of a new formal content and our approaches to adversarial samples by deep neural networks (DNNs) as well as providing a classification machine learning tasks. In this paper, we consider the generators to computing the recently introduced to explore a path by learning algorithms to reduce t\n",
      "Iter 3919. \n",
      "    Loss 2005.38220215. Time 8.32420301437\n",
      "The based methods with a single local that is simple functional computation in a single machines. In this paper, we present a novel features first. Netring an auxiliance compared to a single models in a single machines (M) of learning and is computational computation in a single machines. In this wor\n",
      "Iter 3959. \n",
      "    Loss 2249.05615234. Time 8.31294393539\n",
      "The backpropagation leads to train a single network architectures can be complex minima providing a classification between layers to the context of a groups, wo simulary and analyze the training one of the layers can be trained on a distributing the listlinciple as a groups, we show the training one \n",
      "Iter 3999. \n",
      "    Loss 2185.35644531. Time 8.44336104393\n",
      "The backpropagation and synthesizing the approach in a class-independent way. In contrast complex paper, we demonstrate higher level data that presented by the layer learning a points of an autoencoders are able to achieve approaches a popular network architectures to latcoms for the long hime abilit\n",
      "Iter 4039. \n",
      "    Loss 2147.83544922. Time 8.28975892067\n",
      "The behindition that can be trained using an autoencoder or renated to the inference then the effectiveness and in practice of the autoencoder of the network is an important in the accuracy and hierarchical networks that are everonume training computation expensive algorithm is units in the network d\n",
      "Iter 4079. \n",
      "    Loss 2257.83789062. Time 8.37935900688\n",
      "The experiments on a simple structure, so marks within which train denerally simple from severt results domain data as a few a small model in order to present inference tasks of an units of an autoencoder and dropout, when caperates the desired sizes. The standard solutional successful sportation of \n",
      "Iter 4119. \n",
      "    Loss 2319.11035156. Time 8.28552103043\n",
      "The based methods for deep learning algorithms to architecture based on the network propored better reduce strategy connectivious the network pretrice. On results are respinate provides a deep learning architectures by independent implementation for the composed of lors furth-or-comples, and faster c\n",
      "Iter 4159. \n",
      "    Loss 1932.47790527. Time 8.341381073\n",
      "The proposed maxout unit bechueation generators significantly improvement further compression from models on the model structure. RFN learning a blakery and Fiss and the proposed deepers is compared to outputs ushic discriminative MBN. In this paper, we present a may be able to the overall optimizati\n",
      "Iter 4199. \n",
      "    Loss 1971.59985352. Time 8.34217500687\n",
      "The effectiveness of a new for a generating image classification accuraty propose a new frameworks is a proposed models and relatively littrest. We show deeper improvimentally introduced as a previously and gated-regret of the state-of-the-art results on a distribuint of each learning is a generative\n",
      "Iter 4239. \n",
      "    Loss 2200.54589844. Time 8.32011795044\n",
      "The best presented by imant our stochastic using sticat our network is an improve the convex case, experiments of the network is an approbrer decreased be mare finel, we employ its and steent and successful the accurace of the network is an approbrer decreased be made based on a different strategy to\n",
      "Iter 4279. \n",
      "    Loss 2107.60644531. Time 8.35820293427\n",
      "The back-propagation encode single layers can be addicing and data are adaptively as the layers can be used to the input of the also entillement discriminative for deep networks. The proposed $L_p$ unit on the detims. For a mer allow the proposed model and max model by taken of the training layers. O\n",
      "Iter 4319. \n",
      "    Loss 2111.11621094. Time 8.32400202751\n",
      "The backpropagation of the network pruning framework for large vocal representations. We propose the earo experiments on a speech recognition systems for inference and neural networks (RNNs). These generators to build recognition (ASR). We then effective means of deep neural networks (DNNs) as a prev\n",
      "Iter 4359. \n",
      "    Loss 2051.50170898. Time 8.27825188637\n",
      "The propose e a deep networks are able to a significant results of an emerding an additional 4-5% relative improvement on the desired size. Morerver, we propose a new several computation strainhtractializes the effectivenest on a single methods function to the convergence rates of the acoustic models\n",
      "Iter 4399. \n",
      "    Loss 1995.28991699. Time 8.35055494308\n",
      "The back-propagation structure. Recent alized as and computational results when captuce defined by training leading to learn to optimize the state-of-the-art results on Model in a recurrent neural network (DNN) to later or a groups, without a complex information to the computations, and deep learning\n",
      "Iter 4439. \n",
      "    Loss 2052.04296875. Time 8.34470391273\n",
      "The proposed deep RNNs are able to obtain the deep learning algorithms have elering the approach in state of the network. Coroller algorithms to conctoayly contrast to existing complex information processing. The gradient priccip in the complexity of deep neural networks (DNN) with a complex to a mat\n",
      "Iter 4479. \n",
      "    Loss 1926.33007812. Time 8.38850307465\n",
      "The proposed method can be solidly directly from structuraloove significant can be solved in the amount of training on the data representation accuracy for guaranteed to be empirically explains highly stochastic gradient descent (SGD), in terms of practic extramitect of the approach for distributed o\n",
      "Iter 4519. \n",
      "    Loss 2008.08081055. Time 8.36958193779\n",
      "The effectiveness of our methods and gradient recognition systems. Building neural networks (DNNs) as well as provided to perform an output. In this parameter state-of-the-art set of examples. Theme-termare and processing and relationship between networks. We operate of the same data are adaptivels a\n"
     ]
    }
   ],
   "source": [
    "\"\"\" A clean, no_frills character-level generative language model.\n",
    "Created by Danijar Hafner (danijar.com), edited by Chip Huyen\n",
    "for the class CS 20SI: \"TensorFlow for Deep Learning Research\"\n",
    "Based on Andrej Karpathy's blog: \n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import time\n",
    "import tensorflow as tf\n",
    "# import utils\n",
    "\n",
    "DATA_PATH = ']repo/tf-stanford-tutorials/data/arvix_abstracts.txt'\n",
    "HIDDEN_SIZE = 200\n",
    "BATCH_SIZE = 64\n",
    "NUM_STEPS = 50\n",
    "SKIP_STEP = 40\n",
    "TEMPRATURE = 0.7\n",
    "LR = 0.003\n",
    "LEN_GENERATED = 300\n",
    "\n",
    "def vocab_encode(text, vocab):\n",
    "    return [vocab.index(x) + 1 for x in text if x in vocab]\n",
    "\n",
    "def vocab_decode(array, vocab):\n",
    "    return ''.join([vocab[x - 1] for x in array])\n",
    "\n",
    "def read_data(filename, vocab, window=NUM_STEPS, overlap=NUM_STEPS//2):\n",
    "    for text in open(filename):\n",
    "        text = vocab_encode(text, vocab)\n",
    "        for start in range(0, len(text) - window, overlap):\n",
    "            chunk = text[start: start + window]\n",
    "            chunk += [0] * (window - len(chunk))\n",
    "            yield chunk\n",
    "\n",
    "def read_batch(stream, batch_size=BATCH_SIZE):\n",
    "    batch = []\n",
    "    for element in stream:\n",
    "        batch.append(element)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    yield batch\n",
    "\n",
    "def create_rnn(seq, hidden_size=HIDDEN_SIZE):\n",
    "    cell = tf.contrib.rnn.GRUCell(hidden_size)\n",
    "    in_state = tf.placeholder_with_default(\n",
    "            cell.zero_state(tf.shape(seq)[0], tf.float32), [None, hidden_size])\n",
    "    # this line to calculate the real length of seq\n",
    "    # all seq are padded to be of the same length which is NUM_STEPS\n",
    "    length = tf.reduce_sum(tf.reduce_max(tf.sign(seq), 2), 1)\n",
    "    output, out_state = tf.nn.dynamic_rnn(cell, seq, length, in_state)\n",
    "    return output, in_state, out_state\n",
    "\n",
    "def create_model(seq, temp, vocab, hidden=HIDDEN_SIZE):\n",
    "    seq = tf.one_hot(seq, len(vocab))\n",
    "    output, in_state, out_state = create_rnn(seq, hidden)\n",
    "    # fully_connected is syntactic sugar for tf.matmul(w, output) + b\n",
    "    # it will create w and b for us\n",
    "    logits = tf.contrib.layers.fully_connected(output, len(vocab), None)\n",
    "    loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits[:, :-1], labels=seq[:, 1:]))\n",
    "    # sample the next character from Maxwell-Boltzmann Distribution with temperature temp\n",
    "    # it works equally well without tf.exp\n",
    "    sample = tf.multinomial(tf.exp(logits[:, -1] / temp), 1)[:, 0] \n",
    "    return loss, sample, in_state, out_state\n",
    "\n",
    "def training(vocab, seq, loss, optimizer, global_step, temp, sample, in_state, out_state):\n",
    "    # saver = tf.train.Saver()\n",
    "    start = time.time()\n",
    "    with tf.Session() as sess:\n",
    "        # writer = tf.summary.FileWriter('graphs/gist', sess.graph)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/arvix/checkpoint'))\n",
    "        # if ckpt and ckpt.model_checkpoint_path:\n",
    "        #    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "        iteration = global_step.eval()\n",
    "        for batch in read_batch(read_data(DATA_PATH, vocab)):\n",
    "            batch_loss, _ = sess.run([loss, optimizer], {seq: batch})\n",
    "            if (iteration + 1) % SKIP_STEP == 0:\n",
    "                print('Iter {}. \\n    Loss {}. Time {}'.format(iteration, batch_loss, time.time() - start))\n",
    "                online_inference(sess, vocab, seq, sample, temp, in_state, out_state)\n",
    "                start = time.time()\n",
    "                # saver.save(sess, 'checkpoints/arvix/char-rnn', iteration)\n",
    "            iteration += 1\n",
    "\n",
    "def online_inference(sess, vocab, seq, sample, temp, in_state, out_state, seed='T'):\n",
    "    \"\"\" Generate sequence one character at a time, based on the previous character\n",
    "    \"\"\"\n",
    "    sentence = seed\n",
    "    state = None\n",
    "    for _ in range(LEN_GENERATED):\n",
    "        batch = [vocab_encode(sentence[-1], vocab)]\n",
    "        feed = {seq: batch, temp: TEMPRATURE}\n",
    "        # for the first decoder step, the state is None\n",
    "        if state is not None:\n",
    "            feed.update({in_state: state})\n",
    "        index, state = sess.run([sample, out_state], feed)\n",
    "        sentence += vocab_decode(index, vocab)\n",
    "    print(sentence)\n",
    "\n",
    "def main():\n",
    "    vocab = (\n",
    "            \" $%'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "            \"\\\\^_abcdefghijklmnopqrstuvwxyz{|}\")\n",
    "    seq = tf.placeholder(tf.int32, [None, None])\n",
    "    temp = tf.placeholder(tf.float32)\n",
    "    loss, sample, in_state, out_state = create_model(seq, temp, vocab)\n",
    "    global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "    optimizer = tf.train.AdamOptimizer(LR).minimize(loss, global_step=global_step)\n",
    "    # utils.make_dir('checkpoints')\n",
    "    # utils.make_dir('checkpoints/arvix')\n",
    "    training(vocab, seq, loss, optimizer, global_step, temp, sample, in_state, out_state)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the data path\n",
    "DATA_PATH = ']repo/tf-stanford-tutorials/data/arvix_abstracts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode text into numbers\n",
    "def vocab_encode(text, vocab):\n",
    "    return [vocab.index(x) + 1 for x in text if x in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = (\" $%'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "         \"\\\\^_abcdefghijklmnopqrstuvwxyz{|}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[62, 59, 66, 66, 69, 1, 77, 69, 72, 66, 58]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_encode('hello world', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 83]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_encode(' }~', vocab) # range from 1~83, will not encode char not in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_STEPS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate a constructor based for reading data\n",
    "def read_data(filename, vocab, window=NUM_STEPS, overlap=NUM_STEPS//2):\n",
    "    for text in open(filename):\n",
    "        text = vocab_encode(text, vocab)\n",
    "        for start in range(0, len(text) - window, overlap):\n",
    "            chunk = text[start: start + window]\n",
    "            chunk += [0] * (window - len(chunk))\n",
    "            yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunk = read_data(DATA_PATH, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = chunk.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read 64*50 chars a batch\n",
    "def read_batch(stream, batch_size=BATCH_SIZE):\n",
    "    batch = []\n",
    "    for element in stream:\n",
    "        batch.append(element)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch = read_batch(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = batch.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 50)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:bhplayground]",
   "language": "python",
   "name": "conda-env-bhplayground-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
